"""
è‡ªç„¶è¯­è¨€å¤„ç†(NLP)åŸºç¡€æ•™ç¨‹
ä»Javaå¼€å‘è€…è§’åº¦ç†è§£NLPæ ¸å¿ƒæ¦‚å¿µ
"""

import nltk
import jieba
from collections import Counter
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from transformers import pipeline, AutoTokenizer, AutoModel

class NLPFundamentals:
    """NLPåŸºç¡€æ¦‚å¿µå’Œå®ç°"""
    
    def __init__(self):
        # ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
        except:
            print("NLTKæ•°æ®ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
    
    def text_preprocessing(self, text):
        """æ–‡æœ¬é¢„å¤„ç†ç¤ºä¾‹"""
        print("=== æ–‡æœ¬é¢„å¤„ç† ===")
        
        # åŸå§‹æ–‡æœ¬
        print(f"åŸå§‹æ–‡æœ¬: {text}")
        
        # 1. åˆ†è¯
        chinese_text = "æˆ‘çˆ±å­¦ä¹ äººå·¥æ™ºèƒ½æŠ€æœ¯"
        chinese_tokens = list(jieba.cut(chinese_text))
        print(f"ä¸­æ–‡åˆ†è¯: {chinese_tokens}")
        
        english_text = "I love learning artificial intelligence"
        english_tokens = nltk.word_tokenize(english_text.lower())
        print(f"è‹±æ–‡åˆ†è¯: {english_tokens}")
        
        # 2. å»é™¤åœç”¨è¯
        from nltk.corpus import stopwords
        stop_words = set(stopwords.words('english'))
        filtered_tokens = [word for word in english_tokens if word not in stop_words]
        print(f"å»é™¤åœç”¨è¯å: {filtered_tokens}")
        
        # 3. è¯å¹²æå–
        from nltk.stem import PorterStemmer
        stemmer = PorterStemmer()
        stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]
        print(f"è¯å¹²æå–å: {stemmed_tokens}")
        
        return chinese_tokens, english_tokens
    
    def text_representation(self, texts):
        """æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•"""
        print("\n=== æ–‡æœ¬è¡¨ç¤ºæ–¹æ³• ===")
        
        # 1. è¯è¢‹æ¨¡å‹(Bag of Words)
        print("1. è¯è¢‹æ¨¡å‹:")
        vocab = set()
        for text in texts:
            tokens = jieba.cut(text)
            vocab.update(tokens)
        
        vocab_list = sorted(list(vocab))
        print(f"è¯æ±‡è¡¨: {vocab_list[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ª
        
        # 2. TF-IDFè¡¨ç¤º
        print("\n2. TF-IDFå‘é‡åŒ–:")
        vectorizer = TfidfVectorizer(max_features=10)
        tfidf_matrix = vectorizer.fit_transform(texts)
        print(f"TF-IDFçŸ©é˜µå½¢çŠ¶: {tfidf_matrix.shape}")
        print(f"ç‰¹å¾åç§°: {vectorizer.get_feature_names_out()[:5]}")
        
        return tfidf_matrix
    
    def sentiment_analysis_demo(self):
        """æƒ…æ„Ÿåˆ†æç¤ºä¾‹"""
        print("\n=== æƒ…æ„Ÿåˆ†ææ¼”ç¤º ===")
        
        # å‡†å¤‡ç¤ºä¾‹æ•°æ®
        train_texts = [
            "è¿™ä¸ªäº§å“éå¸¸å¥½ç”¨ï¼Œæˆ‘å¾ˆæ»¡æ„",
            "æœåŠ¡è´¨é‡å¾ˆå·®ï¼Œä¸æ¨è",
            "æ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—è´­ä¹°",
            "ç‰©æµå¤ªæ…¢äº†ï¼Œä½“éªŒä¸å¥½",
            "åŠŸèƒ½å¼ºå¤§ï¼Œæ“ä½œç®€å•"
        ]
        train_labels = [1, 0, 1, 0, 1]  # 1:æ­£é¢, 0:è´Ÿé¢
        
        # åˆ›å»ºæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ç®¡é“
        classifier = Pipeline([
            ('tfidf', TfidfVectorizer()),
            ('clf', MultinomialNB())
        ])
        
        # è®­ç»ƒæ¨¡å‹
        classifier.fit(train_texts, train_labels)
        
        # æµ‹è¯•é¢„æµ‹
        test_texts = ["äº§å“è´¨é‡ä¸é”™", "æœåŠ¡æ€åº¦æ¶åŠ£"]
        predictions = classifier.predict(test_texts)
        
        for text, pred in zip(test_texts, predictions):
            sentiment = "æ­£é¢" if pred == 1 else "è´Ÿé¢"
            print(f"'{text}' -> {sentiment}")
        
        return classifier

class AdvancedNLP:
    """é«˜çº§NLPæŠ€æœ¯"""
    
    def transformer_models(self):
        """Transformeræ¨¡å‹ä»‹ç»"""
        print("\n=== Transformeræ¨¡å‹ ===")
        
        # ä½¿ç”¨Hugging Face transformers
        try:
            # ä¸­æ–‡æƒ…æ„Ÿåˆ†æ
            sentiment_analyzer = pipeline(
                "sentiment-analysis",
                model="uer/roberta-base-finetuned-chinanews-chinese"
            )
            
            # æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹
            texts = ["ä»Šå¤©å¤©æ°”çœŸå¥½", "è¿™ä¸ªç”µå½±å¤ªæ— èŠäº†"]
            results = sentiment_analyzer(texts)
            
            for text, result in zip(texts, results):
                print(f"'{text}' -> {result['label']} (ç½®ä¿¡åº¦: {result['score']:.4f})")
                
        except Exception as e:
            print(f"Transformeræ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯·å…ˆå®‰è£…: pip install transformers torch")
    
    def named_entity_recognition(self):
        """å‘½åå®ä½“è¯†åˆ«"""
        print("\n=== å‘½åå®ä½“è¯†åˆ« ===")
        
        try:
            # ä½¿ç”¨é¢„è®­ç»ƒçš„NERæ¨¡å‹
            ner_pipeline = pipeline("ner", 
                                  model="dbmdz/bert-large-cased-finetuned-conll03-english")
            
            text = "Apple Inc. was founded by Steve Jobs in California."
            entities = ner_pipeline(text)
            
            print(f"åŸæ–‡æœ¬: {text}")
            print("è¯†åˆ«çš„å®ä½“:")
            for entity in entities:
                print(f"  {entity['word']}: {entity['entity']}")
                
        except Exception as e:
            print(f"NERæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

class NLPApplications:
    """NLPå®é™…åº”ç”¨"""
    
    def chatbot_demo(self):
        """èŠå¤©æœºå™¨äººåŸºç¡€"""
        print("\n=== ç®€å•èŠå¤©æœºå™¨äºº ===")
        
        class SimpleChatbot:
            def __init__(self):
                self.responses = {
                    "ä½ å¥½": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ",
                    "å†è§": "å†è§ï¼ç¥ä½ æœ‰ç¾å¥½çš„ä¸€å¤©ï¼",
                    "è°¢è°¢": "ä¸å®¢æ°”ï¼",
                    "é»˜è®¤": "æˆ‘ä¸å¤ªæ˜ç™½ä½ çš„æ„æ€ï¼Œèƒ½æ¢ä¸ªè¯´æ³•å—ï¼Ÿ"
                }
            
            def get_response(self, user_input):
                # ç®€å•çš„å…³é”®è¯åŒ¹é…
                for key in self.responses:
                    if key in user_input:
                        return self.responses[key]
                return self.responses["é»˜è®¤"]
        
        bot = SimpleChatbot()
        
        # æ¼”ç¤ºå¯¹è¯
        test_inputs = ["ä½ å¥½", "è°¢è°¢ä½ ", "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"]
        for inp in test_inputs:
            response = bot.get_response(inp)
            print(f"ç”¨æˆ·: {inp}")
            print(f"æœºå™¨äºº: {response}\n")
    
    def text_similarity(self):
        """æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—"""
        print("\n=== æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®— ===")
        
        from sklearn.metrics.pairwise import cosine_similarity
        
        texts = [
            "äººå·¥æ™ºèƒ½æ˜¯æœªæ¥çš„è¶‹åŠ¿",
            "æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦åˆ†æ”¯",
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
        ]
        
        # TF-IDFå‘é‡åŒ–
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(texts)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity_matrix = cosine_similarity(tfidf_matrix)
        
        print("æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ:")
        for i, text1 in enumerate(texts):
            for j, text2 in enumerate(texts):
                if i < j:  # é¿å…é‡å¤è®¡ç®—
                    similarity = similarity_matrix[i][j]
                    print(f"'{text1}' å’Œ '{text2}': {similarity:.4f}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– è‡ªç„¶è¯­è¨€å¤„ç†(NLP)åŸºç¡€æ•™ç¨‹")
    print("=" * 50)
    
    # åŸºç¡€NLP
    nlp_basic = NLPFundamentals()
    
    # æ–‡æœ¬é¢„å¤„ç†æ¼”ç¤º
    sample_text = "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œ"
    chinese_tokens, english_tokens = nlp_basic.text_preprocessing(sample_text)
    
    # æ–‡æœ¬è¡¨ç¤º
    sample_texts = [
        "æˆ‘å–œæ¬¢å­¦ä¹ æ–°æŠ€æœ¯",
        "äººå·¥æ™ºèƒ½å¾ˆæœ‰è¶£",
        "ç¼–ç¨‹æ˜¯æˆ‘çš„çˆ±å¥½"
    ]
    tfidf_result = nlp_basic.text_representation(sample_texts)
    
    # æƒ…æ„Ÿåˆ†æ
    sentiment_classifier = nlp_basic.sentiment_analysis_demo()
    
    # é«˜çº§NLP
    advanced_nlp = AdvancedNLP()
    advanced_nlp.transformer_models()
    advanced_nlp.named_entity_recognition()
    
    # åº”ç”¨ç¤ºä¾‹
    applications = NLPApplications()
    applications.chatbot_demo()
    applications.text_similarity()
    
    print("\nâœ… NLPåŸºç¡€å­¦ä¹ å®Œæˆï¼")

if __name__ == "__main__":
    main()